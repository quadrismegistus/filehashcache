{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,'..')\n",
    "from hashstash import *\n",
    "logger.setLevel(logging.INFO)\n",
    "from hashstash.profilers.engine_profiler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashStash(engine='memory').profiler.profile_simple(_force=True, data_types=['pandas_df'], size=1_000, num_proc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df=HashStash(engine='dataframe').profile(num_proc=10, size=10_000, data_types=['pandas_df'], iterations=10000)\n",
    "# # df\n",
    "# HashStashProfiler.run_profiles(iterations=10000, size=1_000, num_procs=[1,8], progress_inner=False,progress=True, data_types=['pandas_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=HashStashProfiler.run_profiles(iterations=10000, size=1_000, num_procs=[1], progress_inner=True,progress=True, data_types=['pandas_df'])\n",
    "# df=HashStashProfiler.run_profiles(iterations=10000, size=1_000, num_procs=[1], progress_inner=True,progress=True, data_types=['dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(obj):\n",
    "    d=obj.get('kwargs')\n",
    "    try:\n",
    "        if d['serializer'] == 'pickle': return False\n",
    "        if d['num_proc'] != 1: return False\n",
    "        if d['size']<1000: return False\n",
    "        # if d['size']<1000: return False\n",
    "    except KeyError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "f=HashStashProfiler.profile\n",
    "fstash = f.stash\n",
    "o = []\n",
    "import pandas as pd\n",
    "for key,val in fstash.query(query,return_vals=True):\n",
    "    o.append(val)\n",
    "odf=pd.concat(o) if o else pd.DataFrame()\n",
    "odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1.80s] querying by key = query and value = bool\u001b[32m: 100%|██████████| 18/18 [00:00<00:00, 134.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Num Proc</th>\n",
       "      <th>Cumulative Time (s)</th>\n",
       "      <th>Cumulative Size (B)</th>\n",
       "      <th>Cumulative Rate (it/s)</th>\n",
       "      <th>Cumulative Speed (MB/s)</th>\n",
       "      <th>Engine</th>\n",
       "      <th>Serializer</th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Size (B)</th>\n",
       "      <th>Raw Size (B)</th>\n",
       "      <th>Cached Size (B)</th>\n",
       "      <th>Compression Ratio (%)</th>\n",
       "      <th>Operation</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Rate (it/s)</th>\n",
       "      <th>Speed (MB/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.114432</td>\n",
       "      <td>14244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118708</td>\n",
       "      <td>memory</td>\n",
       "      <td>hashstash</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>14244</td>\n",
       "      <td>11008</td>\n",
       "      <td>77.281662</td>\n",
       "      <td>Serialize</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>70.214008</td>\n",
       "      <td>0.953797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.114434</td>\n",
       "      <td>14244</td>\n",
       "      <td>8.738661</td>\n",
       "      <td>0.118706</td>\n",
       "      <td>memory</td>\n",
       "      <td>hashstash</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>14244</td>\n",
       "      <td>11008</td>\n",
       "      <td>77.281662</td>\n",
       "      <td>Deserialize</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>550.650387</td>\n",
       "      <td>7.480110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.114435</td>\n",
       "      <td>14244</td>\n",
       "      <td>17.476995</td>\n",
       "      <td>0.118705</td>\n",
       "      <td>memory</td>\n",
       "      <td>hashstash</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>14244</td>\n",
       "      <td>11008</td>\n",
       "      <td>77.281662</td>\n",
       "      <td>Encode</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>2376.376204</td>\n",
       "      <td>32.281020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.114437</td>\n",
       "      <td>14244</td>\n",
       "      <td>26.215328</td>\n",
       "      <td>0.118704</td>\n",
       "      <td>memory</td>\n",
       "      <td>hashstash</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>14244</td>\n",
       "      <td>11008</td>\n",
       "      <td>77.281662</td>\n",
       "      <td>Decode</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>9341.434298</td>\n",
       "      <td>126.895323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.114437</td>\n",
       "      <td>14244</td>\n",
       "      <td>34.953480</td>\n",
       "      <td>0.118703</td>\n",
       "      <td>memory</td>\n",
       "      <td>hashstash</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>14244</td>\n",
       "      <td>11008</td>\n",
       "      <td>77.281662</td>\n",
       "      <td>Set</td>\n",
       "      <td>0.027786</td>\n",
       "      <td>35.989326</td>\n",
       "      <td>0.488884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>7.912302</td>\n",
       "      <td>1491803</td>\n",
       "      <td>87.837901</td>\n",
       "      <td>0.179808</td>\n",
       "      <td>lmdb</td>\n",
       "      <td>jsonpickle</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>12856</td>\n",
       "      <td>9544</td>\n",
       "      <td>74.237710</td>\n",
       "      <td>Encode</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>4951.952774</td>\n",
       "      <td>60.713105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>7.912303</td>\n",
       "      <td>1491803</td>\n",
       "      <td>87.964273</td>\n",
       "      <td>0.179808</td>\n",
       "      <td>lmdb</td>\n",
       "      <td>jsonpickle</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>12856</td>\n",
       "      <td>9544</td>\n",
       "      <td>74.237710</td>\n",
       "      <td>Decode</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>11335.956757</td>\n",
       "      <td>138.983784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>7.912304</td>\n",
       "      <td>1491803</td>\n",
       "      <td>88.090648</td>\n",
       "      <td>0.179808</td>\n",
       "      <td>lmdb</td>\n",
       "      <td>jsonpickle</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>12856</td>\n",
       "      <td>9544</td>\n",
       "      <td>74.237710</td>\n",
       "      <td>Set</td>\n",
       "      <td>0.015739</td>\n",
       "      <td>63.535621</td>\n",
       "      <td>0.778974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>7.912305</td>\n",
       "      <td>1491803</td>\n",
       "      <td>88.217023</td>\n",
       "      <td>0.179808</td>\n",
       "      <td>lmdb</td>\n",
       "      <td>jsonpickle</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>12856</td>\n",
       "      <td>9544</td>\n",
       "      <td>74.237710</td>\n",
       "      <td>Get</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>15887.515152</td>\n",
       "      <td>194.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>7.912305</td>\n",
       "      <td>1491803</td>\n",
       "      <td>88.343408</td>\n",
       "      <td>0.179808</td>\n",
       "      <td>lmdb</td>\n",
       "      <td>jsonpickle</td>\n",
       "      <td>zlib+b64</td>\n",
       "      <td>DataFrame</td>\n",
       "      <td>10000</td>\n",
       "      <td>12856</td>\n",
       "      <td>9544</td>\n",
       "      <td>74.237710</td>\n",
       "      <td>Total</td>\n",
       "      <td>0.015454</td>\n",
       "      <td>64.708939</td>\n",
       "      <td>0.793360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iteration  Num Proc  Cumulative Time (s)  Cumulative Size (B)  \\\n",
       "0            0         1             0.114432                14244   \n",
       "1            0         1             0.114434                14244   \n",
       "2            0         1             0.114435                14244   \n",
       "3            0         1             0.114437                14244   \n",
       "4            0         1             0.114437                14244   \n",
       "..         ...       ...                  ...                  ...   \n",
       "695         99         1             7.912302              1491803   \n",
       "696         99         1             7.912303              1491803   \n",
       "697         99         1             7.912304              1491803   \n",
       "698         99         1             7.912305              1491803   \n",
       "699         99         1             7.912305              1491803   \n",
       "\n",
       "     Cumulative Rate (it/s)  Cumulative Speed (MB/s)  Engine  Serializer  \\\n",
       "0                  0.000000                 0.118708  memory   hashstash   \n",
       "1                  8.738661                 0.118706  memory   hashstash   \n",
       "2                 17.476995                 0.118705  memory   hashstash   \n",
       "3                 26.215328                 0.118704  memory   hashstash   \n",
       "4                 34.953480                 0.118703  memory   hashstash   \n",
       "..                      ...                      ...     ...         ...   \n",
       "695               87.837901                 0.179808    lmdb  jsonpickle   \n",
       "696               87.964273                 0.179808    lmdb  jsonpickle   \n",
       "697               88.090648                 0.179808    lmdb  jsonpickle   \n",
       "698               88.217023                 0.179808    lmdb  jsonpickle   \n",
       "699               88.343408                 0.179808    lmdb  jsonpickle   \n",
       "\n",
       "     Encoding  Data Type  Size (B)  Raw Size (B)  Cached Size (B)  \\\n",
       "0    zlib+b64  DataFrame     10000         14244            11008   \n",
       "1    zlib+b64  DataFrame     10000         14244            11008   \n",
       "2    zlib+b64  DataFrame     10000         14244            11008   \n",
       "3    zlib+b64  DataFrame     10000         14244            11008   \n",
       "4    zlib+b64  DataFrame     10000         14244            11008   \n",
       "..        ...        ...       ...           ...              ...   \n",
       "695  zlib+b64  DataFrame     10000         12856             9544   \n",
       "696  zlib+b64  DataFrame     10000         12856             9544   \n",
       "697  zlib+b64  DataFrame     10000         12856             9544   \n",
       "698  zlib+b64  DataFrame     10000         12856             9544   \n",
       "699  zlib+b64  DataFrame     10000         12856             9544   \n",
       "\n",
       "     Compression Ratio (%)    Operation  Time (s)   Rate (it/s)  Speed (MB/s)  \n",
       "0                77.281662    Serialize  0.014242     70.214008      0.953797  \n",
       "1                77.281662  Deserialize  0.001816    550.650387      7.480110  \n",
       "2                77.281662       Encode  0.000421   2376.376204     32.281020  \n",
       "3                77.281662       Decode  0.000107   9341.434298    126.895323  \n",
       "4                77.281662          Set  0.027786     35.989326      0.488884  \n",
       "..                     ...          ...       ...           ...           ...  \n",
       "695              74.237710       Encode  0.000202   4951.952774     60.713105  \n",
       "696              74.237710       Decode  0.000088  11335.956757    138.983784  \n",
       "697              74.237710          Set  0.015739     63.535621      0.778974  \n",
       "698              74.237710          Get  0.000063  15887.515152    194.787879  \n",
       "699              74.237710        Total  0.015454     64.708939      0.793360  \n",
       "\n",
       "[12600 rows x 18 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[4.00s] [1x]\u001b[32m: 100%|██████████| 18/18 [00:00<00:00, 139.35it/s]\n"
     ]
    }
   ],
   "source": [
    "df=HashStashProfiler.run_profiles(iterations=100, size=10_000, num_procs=[1], progress_inner=True,progress=True, data_types=['pandas_df'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# fig+=p9.facet_grid('Data Type ~ Size (B)',scales='free')\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# fig+=p9.scale_y_log10()\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# fig+=p9.scale_x_log10()\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fig\n\u001b[0;32m---> 41\u001b[0m \u001b[43mplot_iterations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m, in \u001b[0;36mplot_iterations\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m fig\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mp9\u001b[38;5;241m.\u001b[39mgeom_line()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# fig+=p9.geom_smooth()\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m fig\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mp9\u001b[38;5;241m.\u001b[39mgeom_text(p9\u001b[38;5;241m.\u001b[39maes(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEngine\u001b[39m\u001b[38;5;124m'\u001b[39m), data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdfx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfigdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEngine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSize (B)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# fig+=p9.geom_point()\u001b[39;00m\n\u001b[1;32m     31\u001b[0m fig\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mp9\u001b[38;5;241m.\u001b[39mtheme_classic()\n",
      "File \u001b[0;32m~/github/hashstash/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/github/hashstash/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/github/hashstash/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import plotnine as p9\n",
    "# p9.options.figure_size = (12,8)\n",
    "\n",
    "# def plot_iterations(df):\n",
    "#     df['Iteration Group'] = df['Iteration']# // 1000 * 1000\n",
    "\n",
    "\n",
    "#     # df=df[df.Operation.isin({'Total'})]\n",
    "#     df=df[df.Operation.isin({'Total'})]\n",
    "#     df=df[df['Data Type'].isin({'DataFrame'})]\n",
    "#     # df=df[df.Serializer.isin({'hashstash','jsonpickle'})]\n",
    "#     # df = df[(df['Num Proc'].isin({1}))]\n",
    "#     # df = df[(df['Data Type'].isin({'DataFrame'}))]\n",
    "#     df = df.query('1000<Iteration')\n",
    "\n",
    "#     figdf = df.groupby(['Iteration Group','Engine','Data Type', 'Size (B)']).median(numeric_only=True).reset_index()\n",
    "#     figdf['Cumulative Size (MB)'] = figdf['Cumulative Size (B)'] / 1024 / 1024\n",
    "#     figdf['Cumulative Size (GB)'] = figdf['Cumulative Size (MB)'] / 1024\n",
    "#     # figdf = figdf[(figdf['Cumulative Size (MB)'] > 1) & (figdf['Cumulative Size (MB)'] < 5)]\n",
    "#     # engine_speed_avg = df.groupby('Engine')['Speed (MB/s)'].mean().sort_values(ascending=False)\n",
    "#     # figdf['Engine'] = pd.Categorical(figdf['Engine'], categories=engine_speed_avg.index, ordered=True)\n",
    "#     # fig = p9.ggplot(figdf, p9.aes(x='Cumulative Size (MB)',y='Cumulative Speed (MB/s)',color='Engine'))\n",
    "#     fig = p9.ggplot(figdf, p9.aes(x='Iteration Group',y='Cumulative Rate (it/s)',color='Engine'))\n",
    "#     # fig = p9.ggplot(figdf, p9.aes(x='Cumulative Size (GB)',y='Cumulative Speed (MB/s)',color='Engine'))\n",
    "#     # fig = p9.ggplot(figdf, p9.aes(x='Iteration Group',y='Cumulative Time (s)',color='Engine'))\n",
    "#     fig+=p9.geom_line()\n",
    "#     # fig+=p9.geom_smooth()\n",
    "#     fig+=p9.geom_text(p9.aes(label='Engine'), data=pd.concat(dfx.iloc[-1:] for g,dfx in figdf.groupby(['Engine','Data Type', 'Size (B)'])))\n",
    "#     # fig+=p9.geom_point()\n",
    "#     fig+=p9.theme_classic()\n",
    "#     # fig+=p9.facet_wrap('Data Type', scales='free',ncol=1)\n",
    "#     # fig+=p9.facet_wrap('Serializer', nrow=1)\n",
    "#     fig+=p9.facet_wrap('Size (B)', nrow=1)\n",
    "#     # fig+=p9.facet_grid('Data Type ~ Size (B)',scales='free')\n",
    "#     # fig+=p9.scale_y_log10()\n",
    "#     # fig+=p9.scale_x_log10()\n",
    "#     return fig\n",
    "\n",
    "\n",
    "# plot_iterations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "\n",
    "\n",
    "def plot_num_proc():\n",
    "    df['Iteration Group'] = df['Iteration'] // 100\n",
    "    figdf = df[df.Operation=='Total'].groupby(['Engine','Num Proc']).median(numeric_only=True).reset_index()\n",
    "    engine_speed_avg = df.groupby('Engine')['Speed (MB/s)'].mean().sort_values(ascending=False)\n",
    "    figdf['Engine'] = pd.Categorical(figdf['Engine'], categories=engine_speed_avg.index, ordered=True)\n",
    "    fig = p9.ggplot(figdf, p9.aes(x='Num Proc',y='Rate (it/s)',color='Engine',label='Engine'))\n",
    "    fig+=p9.geom_line(p9.aes(group='Engine'))\n",
    "    # fig+=p9.geom_point()\n",
    "    fig+=p9.geom_text(size=8, ha='right')\n",
    "    fig+=p9.theme_classic()\n",
    "    fig+=p9.scale_y_log10()\n",
    "    return fig\n",
    "\n",
    "\n",
    "plot_num_proc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = HashStashProfiler(HashStash()).profile\n",
    "f = HashStashProfiler.profile\n",
    "fstash = f.stash\n",
    "\n",
    "\n",
    "fstash\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=HashStashProfiler.run_profiles(iterations=2000, size=1_000, num_proc=1, num_procs=[4], progress_inner=False, progress=True, data_types=['pandas_df'])\n",
    "# engine_speed_avg = df[df.Operation == 'Total'].groupby('Engine')['Speed (MB/s)'].mean().sort_values(ascending=False)\n",
    "# df['Engine'] = pd.Categorical(df['Engine'], categories=engine_speed_avg.index, ordered=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "\n",
    "\n",
    "# def plot_iterations():\n",
    "#     figdf = df.groupby(['Iteration','Engine']).median(numeric_only=True).reset_index()\n",
    "\n",
    "#     fig = p9.ggplot(figdf, p9.aes(x='Iteration',y='Rate (it/s)',color='Engine'))\n",
    "#     fig+=p9.geom_line()\n",
    "#     return fig\n",
    "\n",
    "# plot_iterations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Data Type','Engine']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_types():\n",
    "    figdf = df.groupby(['Data Type','Engine']).median(numeric_only=True).reset_index()\n",
    "    \n",
    "    fig = p9.ggplot(figdf, p9.aes(x='Engine',y='Rate (it/s)',color='Data Type',group='Data Type'))\n",
    "    fig+=p9.geom_point()\n",
    "    fig+=p9.geom_line()\n",
    "    fig+=p9.scale_y_log10()\n",
    "    return fig\n",
    "\n",
    "plot_data_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fstash.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f(iterations=100,num_proc=5, num_procs=[1])\n",
    "# f(iterations=1000,num_proc=1, num_procs=[1,2,4,8], progress_inner=True, engines = [e for e in ENGINES if e not in {'shelve'}])\n",
    "# # f(iterations=100,num_proc=1, num_procs=[5], engines=['memory','pairtree'])\n",
    "\n",
    "# # f(iterations=100,num_proc=1, num_procs=[5], _force=False, engines=['memory','pairtree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Operation=='Total') & (df['Data Type']=='DataFrame')].groupby(['Engine','Serializer']).median(numeric_only=True).sort_values('Speed (MB/s)', ascending=False)[['Speed (MB/s)','Time (s)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf=df[df.Operation.isin({'Total'}) & (df['Serializer']=='hashstash') & (df['Engine']!='memory')]\n",
    "# sdf=df[df.Operation.isin({'Total'}) & (df['Serializer']=='hashstash') & (df['Engine']!='memory') & (df['Data Type']!='DataFrame')]\n",
    "df.groupby(['Engine','Serializer','Num Proc']).median(numeric_only=True).sort_values('Rate (it/s)', ascending=False)[['Speed (MB/s)','Time (s)','Rate (it/s)']].head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = df.groupby(['Engine','Serializer','Num Proc','Operation']).median(numeric_only=True).reset_index()\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg['Num Proc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotnine as p9\n",
    "\n",
    "# df_avg = df_avg[df_avg.Operation.isin({'Get','Set','Total'})]\n",
    "# # df_avg = df_avg[df_avg.Serializer.isin({'hashstash','jsonpickle'})]\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Calculate the average speed for each engine\n",
    "# engine_speed_avg = df_avg[df_avg.Operation == 'Total'].groupby('Engine')['Speed (MB/s)'].mean().sort_values(ascending=False)\n",
    "\n",
    "# # Convert Engine to a categorical type with categories sorted by average speed\n",
    "# df_avg['Engine'] = pd.Categorical(df_avg['Engine'], categories=engine_speed_avg.index, ordered=True)\n",
    "\n",
    "\n",
    "# fig = p9.ggplot(df_avg, p9.aes(x='Engine', y='Speed (MB/s)'))\n",
    "# fig += p9.geom_boxplot() \n",
    "# fig += p9.geom_point(p9.aes(color='Operation', shape='Serializer'),size=3) + p9.theme_classic() + p9.scale_y_log10()\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "p9.options.figure_size = (12,8)\n",
    "df_avg = df.groupby(['Engine','Num Proc','Serializer','Operation']).median(numeric_only=True).reset_index()\n",
    "figdf=df_avg[df_avg.Operation.isin({'Get','Set','Serialize','Deserialize'})]\n",
    "\n",
    "engine_speed_avg = df.groupby('Engine')['Time (s)'].median().sort_values(ascending=True)\n",
    "# # Convert Engine to a categorical type with categories sorted by average speed\n",
    "figdf['Engine'] = pd.Categorical(figdf['Engine'], categories=engine_speed_avg.index, ordered=True)\n",
    "\n",
    "fig = (p9.ggplot(figdf, p9.aes(x='Engine', y='Time (s)',color='Serializer'))\n",
    "       # + p9.geom_boxplot()\n",
    "       + p9.geom_point(size=3)\n",
    "       + p9.geom_line(p9.aes(group='Serializer'))\n",
    "       + p9.facet_wrap('Operation', ncol=2,scales='free')\n",
    "       + p9.theme_classic()\n",
    "       + p9.scale_y_log10()\n",
    "       \n",
    "       )\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "figdf=df_avg[df_avg.Operation.isin({'Total'})]\n",
    "# # Convert Engine to a categorical type with categories sorted by average speed\n",
    "figdf['Engine'] = pd.Categorical(figdf['Engine'], categories=engine_speed_avg.index, ordered=True)\n",
    "\n",
    "fig = (p9.ggplot(figdf, p9.aes(x='Engine', y='Speed (MB/s)'))\n",
    "       + p9.geom_boxplot()\n",
    "       + p9.geom_point(p9.aes(color='Operation', shape='Serializer'), size=3)\n",
    "       + p9.facet_wrap('Num Proc', ncol=1)\n",
    "       + p9.theme_classic()\n",
    "       + p9.scale_y_log10())\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figdf = df.query('Operation==\"Total\"').groupby(['Engine','Serializer','Size']).median(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "fig = (p9.ggplot(figdf, p9.aes(x='Engine', y='Speed (MB/s)'))\n",
    "       + p9.geom_boxplot()\n",
    "       + p9.geom_point(p9.aes(color='Data Type', shape='Serializer'), size=3)\n",
    "       + p9.theme_classic()\n",
    "       # + p9.scale_y_log10()\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Data Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Num Proc'] = df['Num Proc'].apply(lambda x: f'{int(x):02}x')\n",
    "df['Size (KB)'] = df['Raw Size (MB)'].apply(lambda x: int(x*1024))\n",
    "df['Size (B)'] = df['Raw Size (MB)'].apply(lambda x: int(x*1024*1024))\n",
    "\n",
    "import math\n",
    "\n",
    "def group_by_magnitude(size_bytes):\n",
    "    if size_bytes == 0:\n",
    "        return '0 B'\n",
    "    \n",
    "    magnitude = int(math.log10(size_bytes))\n",
    "    unit_index = magnitude // 3\n",
    "    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "    \n",
    "    if unit_index >= len(units):\n",
    "        return f'{size_bytes} B'  # Fallback for extremely large sizes\n",
    "    \n",
    "    unit = units[unit_index]\n",
    "    # return unit\n",
    "    scaled_size = size_bytes / (1024 ** unit_index)\n",
    "    \n",
    "    if magnitude % 3 == 0:\n",
    "        return f'1 {unit}'\n",
    "    elif magnitude % 3 == 1:\n",
    "        return f'10 {unit}'\n",
    "    else:\n",
    "        return f'100 {unit}'\n",
    "\n",
    "# Apply the function to create the new column\n",
    "# df['Size Name'] = df['Size (B)'].apply(lambda x: 10**(round(math.log10(x))//1*1))\n",
    "df['Size Name'] = df['Size (B)'].apply(lambda x: group_by_magnitude(x))\n",
    "# df['Size Name'] = df['Size (B)'].apply(lambda x: x//1000*1000)\n",
    "# df['Size Name'] = df['Size (B)'].apply(lambda x: \"B\" if x<1000 else \"KB\" if x<1000000 else \"MB\")\n",
    "df['Size Name'].value_counts()\n",
    "df=df[df['Size Name'].isin({'1 KB'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speed_stats(group_by:list=None, operations:list=None, carry_over_cols:list=('Speed (MB/s)', 'Time (s)'), also_cols=('Raw Size (MB)',)):\n",
    "    global df\n",
    "    # df = df.reset_index().replace({'':np.nan})\n",
    "    if operations:\n",
    "        df = df[df['Operation'].isin(operations)]\n",
    "    \n",
    "    # Group by Engine and Operation, calculate median for numeric columns\n",
    "    gby = ([] if not group_by else list(group_by)) + ['Operation']\n",
    "    grouped = df.groupby(gby).median(numeric_only=True)\n",
    "    \n",
    "    # Reset index to make Operation a column\n",
    "    grouped = grouped.reset_index()\n",
    "    \n",
    "    # Pivot the table to get speeds and carried over columns as separate columns\n",
    "    pivot_values = list(carry_over_cols)\n",
    "    pivoted = grouped.pivot(index=list(group_by)+list(also_cols), columns='Operation', values=pivot_values)\n",
    "    \n",
    "    # Flatten column names\n",
    "    # pivoted.columns = [f'{col[1]} {col[0]}' if col[0] in carry_over_cols else f'{col[1]} (MB/s)' for col in pivoted.columns]\n",
    "    pivoted.columns = [f'{col[1]} {col[0].split()[-1]}' for col in pivoted.columns]\n",
    "    \n",
    "    # Sort by Total speed descending\n",
    "    odf = pivoted.reset_index()#.sort_values('Total (MB/s)', ascending=False)\n",
    "    for c in odf:\n",
    "        if c.endswith('(s)'):\n",
    "            odf[f'{c.replace(\"(s)\",\"(it/s)\")}'] = 1/odf[c]\n",
    "    odf['GetSet (s)'] = odf['Get (s)'] + odf['Set (s)']\n",
    "    odf['GetSet (it/s)'] = 1/odf['GetSet (s)']\n",
    "    odf['Group Name'] = ['+'.join(row[x] for x in group_by) for i,row in odf.iterrows()]\n",
    "    return odf.set_index('Group Name').sort_values('Total (MB/s)', ascending=False)\n",
    "\n",
    "get_speed_stats(['Engine','Serializer'])['Total (MB/s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and display results\n",
    "get_speed_stats(group_by=['Engine','Size Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "from plotnine import *\n",
    "\n",
    "def plot_speed_comparison(df, group_by=['Engine'], x_col='Write', y_col='Read', color=None, log=False, facet=None):\n",
    "    p9.options.figure_size = (8, 6)\n",
    "    # Prepare the data\n",
    "    if color and not color in group_by: group_by.append(color)\n",
    "    if facet and not facet in group_by: group_by.append(facet)\n",
    "    speed_df = get_speed_stats(group_by=group_by)\n",
    "    speed_df = speed_df.reset_index()\n",
    "    \n",
    "    # Extract the specific operations we want to compare\n",
    "    # cols = [*group_by]\n",
    "    # if not x_col in cols: cols.append(x_col)\n",
    "    # if not y_col in cols: cols.append(y_col)\n",
    "    plot_df = speed_df #[cols]\n",
    "    \n",
    "    # Create a label column that combines all group_by columns\n",
    "    plot_df['Label'] = plot_df[group_by].apply(lambda row: ' '.join(row.values[:1].astype(str)), axis=1)\n",
    "    \n",
    "    # Create the plot\n",
    "    aes_params = {'x': x_col, 'y': y_col, 'label': 'Label'}\n",
    "    if color:\n",
    "        aes_params['color'] = color\n",
    "    \n",
    "    plot = (\n",
    "        ggplot(plot_df, aes(**aes_params))\n",
    "        # + geom_point(size=3)\n",
    "        + geom_text(aes(label='Label'), size=8)\n",
    "        + theme_classic()\n",
    "        + labs(title=f'{y_col} vs {x_col} Speed Comparison')\n",
    "            #    x=f'{x_operation} Speed (MB/s)',\n",
    "            #    y=f'{y_operation} Speed (MB/s)')\n",
    "    )\n",
    "\n",
    "    if log:\n",
    "        plot += scale_x_log10()\n",
    "        plot += scale_y_log10()\n",
    "        # plot += labs(x=f'{x_operation} Speed (MB/s) - Log Scale',\n",
    "                    #  y=f'{y_operation} Speed (MB/s) - Log Scale')\n",
    "\n",
    "    if facet:\n",
    "        plot += facet_wrap(facet,ncol=2, scales='free')\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_plot = plot_speed_comparison(df, group_by=['Engine'], x_col='Read (it/s)', y_col='Write (it/s)', log=True)\n",
    "comparison_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_plot = plot_speed_comparison(df, group_by=['Engine'], x_col='Get (it/s)', y_col='Set (it/s)', log=True)\n",
    "comparison_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_plot = plot_speed_comparison(df, group_by=['Engine'], x_col='Get (it/s)', y_col='Set (it/s)', log=True, color='Data Type')\n",
    "comparison_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_plot = plot_speed_comparison(df, group_by=['Data Type','Serializer'], x_col='Serialize (it/s)', y_col='Deserialize (it/s)',log=True, color='Serializer')\n",
    "comparison_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "comparison_plot = plot_speed_comparison(df, group_by=['Engine','Num Proc'], x_col='Num Proc', y_col='GetSet (it/s)', log=False, color='Engine') + p9.geom_line()\n",
    "comparison_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotnine as p9\n",
    "# comparison_plot = plot_speed_comparison(df, group_by=['Engine','Num Proc'], x_col='Num Proc', y_col='Total (MB/s)', log=False, color='Engine') + p9.geom_line()\n",
    "# comparison_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "comparison_plot = plot_speed_comparison(df, group_by=['Engine','Num Proc'], x_col='Num Proc', y_col='Write (it/s)', log=True, color='Engine') + p9.geom_line() + p9.scale_x_continuous()\n",
    "comparison_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotnine as p9\n",
    "# comparison_plot = plot_speed_comparison(df[df.Serializer=='hashstash'], group_by=['Engine','Num Proc'], x_col='Num Proc', y_col='Set (MB/s)', log=True, color='Engine',facet='Size Name') + p9.geom_line()\n",
    "# comparison_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
