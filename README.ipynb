{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashStash\n",
    "\n",
    "HashStash is a versatile caching library for Python that supports multiple storage engines, serializers, and encoding options. It provides a simple dictionary-like interface for caching data with various backend options. HashStash is designed to be easy to use, flexible, and efficient.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quadrismegistus/hashstash/blob/main/README.ipynb)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Features](#features)\n",
    "  - [Convenient usage](#convenient-usage)\n",
    "  - [Multiple storage engines](#multiple-storage-engines)\n",
    "  - [Multiple serializers](#multiple-serializers)\n",
    "  - [Compression and encoding options](#compression-and-encoding-options)\n",
    "- [Installation](#installation)\n",
    "- [Usage](#usage)\n",
    "  - [Creating a stash](#creating-a-stash)\n",
    "  - [Stashing objects](#stashing-objects)\n",
    "  - [Works like a dictionary](#works-like-a-dictionary)\n",
    "  - [Stashing function results](#stashing-function-results)\n",
    "  - [Mapping functions](#mapping-functions)\n",
    "  - [Assembling DataFrames](#assembling-dataframes)\n",
    "  - [Append mode](#append-mode)\n",
    "  - [Temporary Caches](#temporary-caches)\n",
    "  - [Utilities](#utilities)\n",
    "    - [Serialization](#serialization)\n",
    "    - [Encoding and Compression](#encoding-and-compression)\n",
    "- [Profiling](#profiling)\n",
    "  - [Engines](#engines)\n",
    "  - [Serializers](#serializers)\n",
    "  - [Encodings](#encodings)\n",
    "  - [All together](#all-together)\n",
    "- [Development](#development)\n",
    "  - [Tests](#tests)\n",
    "  - [Contributing](#contributing)\n",
    "  - [License](#license)\n",
    "\n",
    "## Features\n",
    "\n",
    "### Convenient usage\n",
    "- Dictionary-like interface, except absolutely anything can be either a key or value (even unhashable entities like sets or unpicklable entities like lambdas, local functions, etc)\n",
    "\n",
    "- Multiprocessing support: connection pooling and locking parallelize operations as much as the specific engine allows\n",
    "\n",
    "- Functions like `stash.run` and decorators like `@stashed_result` cache the results of function calls\n",
    "\n",
    "- Functions like `stash.map` and `@stash_mapped` parallelize function calls across many objects, with stashed results\n",
    "\n",
    "- Easy dataframe assembly from cached contents\n",
    "\n",
    "### Multiple storage engines\n",
    "\n",
    "- File-based\n",
    "    - \"__pairtree__\" (no dependencies, no database; just organized folder and file structure; very fast)\n",
    "    - \"__[lmdb](https://pypi.org/project/lmdb/)__\" (single file, very efficient, slightly faster than pairtree)\n",
    "    - \"__[diskcache](https://pypi.org/project/diskcache/)__\" (similar to pairtree, but slower)\n",
    "    - \"__sqlite__\" (using [sqlitedict](https://pypi.org/project/sqlitedict/))\n",
    "\n",
    "- Server-based\n",
    "    - \"__redis__\" (using [redis-py](https://pypi.org/project/redis/))\n",
    "    - \"__mongo__\" (using [pymongo](https://pypi.org/project/pymongo/))\n",
    "\n",
    "- In-memory\n",
    "    - \"__memory__\" (shared memory, using [ultradict](https://pypi.org/project/ultradict/))\n",
    "\n",
    "### Multiple serializers\n",
    "\n",
    "- Transportable between Python versions\n",
    "    - \"__hashstash__\"\n",
    "        - Custom, no dependencies\n",
    "        - Can serialize nearly anything, even lambdas or functions defined within functions\n",
    "        - Serializes pandas dataframes using pyarrow if available\n",
    "        - Faster than jsonpickle but with larger file sizes\n",
    "        - Mostly JSON-based, with some binary data\n",
    "    - \"__[jsonpickle](https://pypi.org/project/jsonpickle/)__\"\n",
    "        - Flexible, battle-tested, but slowest\n",
    "\n",
    "- Not transportable between Python versions\n",
    "    - \"__pickle__\"\n",
    "        - Standard library\n",
    "        - By far the fastest\n",
    "        - But dangerous to use when sharing data across projects or Python versions \n",
    "\n",
    "### Compression and encoding options\n",
    "- External compressors (with depedencies):\n",
    "    - \"__[lz4](<https://pypi.org/project/python-lz4/)>)__\" (fastest)\n",
    "    - \"__[blosc](https://pypi.org/project/blosc/)__\"\n",
    "\n",
    "- Built-in compressors (no dependencies):\n",
    "    - \"__zlib__\"\n",
    "    - \"__gzip__\"\n",
    "    - \"__bz2__\" (smallest file size, but slowest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "HashStash requires no dependencies by default, but you can install optional dependencies to get the best performance.\n",
    "\n",
    "* Default installation (no dependencies): `pip install hashstash`\n",
    "\n",
    "* Installation with only the recommended/optimal settings (lmdb engine, lz4 compression, and pyarrow dataframe serialization): `pip install hashstash[rec]`\n",
    "\n",
    "* Full installation with all optional dependencies: `pip install hashstash[all]`\n",
    "\n",
    "* Development installation: `pip install hashstash[dev]`\n",
    "\n",
    "For all options see [pyproject.toml](./pyproject.toml) under [project.optional-dependencies]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU hashstash[rec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Here's a quick example of how to use HashStash. \n",
    "\n",
    "### Creating a stash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PairtreeHashStash(~/.cache/hashstash/project_stash/sub_stash/pairtree.hashstash.lz4+b64/data.db)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root_dir': '/Users/ryan/.cache/hashstash/project_stash',\n",
       " 'dbname': 'sub_stash',\n",
       " 'engine': 'pairtree',\n",
       " 'serializer': 'hashstash',\n",
       " 'compress': 'lz4',\n",
       " 'b64': True,\n",
       " 'append_mode': False,\n",
       " 'is_function_stash': False,\n",
       " 'is_tmp': False,\n",
       " 'filename': 'data.db'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashstash import HashStash\n",
    "\n",
    "# Create a stash instance\n",
    "stash = HashStash()\n",
    "\n",
    "# or customize:\n",
    "stash = HashStash(\n",
    "    # naming\n",
    "    root_dir=\"project_stash\",    # root directory of the stash (default: default_stash)\n",
    "                                 # if not an absolute path, will be ~/.cache/hashstash/[root_dir]\n",
    "    dbname=\"sub_stash\",          # name of \"database\" or subfolder (default: main)\n",
    "    \n",
    "    # engines\n",
    "    engine=\"pairtree\",           # or lmdb, sqlite, diskcache, redis, mongo, or memory\n",
    "    serializer=\"hashstash\",      # or jsonpickle or pickle\n",
    "    compress='lz4',              # or blosc, bz2, gzip, zlib, or raw\n",
    "    b64=True,                    # base64 encode keys and values\n",
    "\n",
    "    # storage options\n",
    "    append_mode=False,           # store all versions of a key/value pair\n",
    "    clear=True                   # clear on init\n",
    ")\n",
    "\n",
    "# show stash type and path\n",
    "print(stash)\n",
    "\n",
    "# show stash config\n",
    "stash.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stashing objects\n",
    "\n",
    "Literally anything can be a key or value, including lambdas, local functions, sets, dataframes, dictionaries, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traditional dictionary keys,,,\n",
    "stash[\"bad\"] = \"cat\"                 # string key\n",
    "stash[(\"bad\",\"good\")] = \"cat\"        # tuple key\n",
    "\n",
    "# ...unhashable keys...\n",
    "stash[{\"goodness\":\"bad\"}] = \"cat\"    # dict key\n",
    "stash[[\"bad\",\"good\"]] = \"cat\"        # list key\n",
    "stash[{\"bad\",\"good\"}] = \"cat\"        # set key\n",
    "\n",
    "# ...func keys...\n",
    "def func_key(x): pass                \n",
    "stash[func_key] = \"cat\"              # function key\n",
    "lambda_key = lambda x: x\n",
    "stash[lambda_key] = \"cat\"           # lambda key\n",
    "\n",
    "# ...very unhashable keys...\n",
    "import pandas as pd\n",
    "df_key = pd.DataFrame(                  \n",
    "    {\"name\":[\"cat\"], \n",
    "     \"goodness\":[\"bad\"]}\n",
    ")\n",
    "stash[df_key] = \"cat\"                # dataframe key  \n",
    "\n",
    "# all should equal \"cat\":\n",
    "(\n",
    "    stash[\"bad\"],\n",
    "    stash[(\"bad\",\"good\")],\n",
    "    stash[{\"goodness\":\"bad\"}],\n",
    "    stash[[\"bad\",\"good\"]],\n",
    "    stash[{\"bad\",\"good\"}],\n",
    "    stash[func_key],\n",
    "    stash[lambda_key],\n",
    "    stash[df_key]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works like a dictionary\n",
    "\n",
    "HashStash fully implements the dictionary's `MutableMapping` interface, providing all its methods, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item #1:\n",
      "{'good', 'bad'} >>> cat\n",
      "\n",
      "Item #2:\n",
      "{'goodness': 'bad'} >>> cat\n",
      "\n",
      "Item #3:\n",
      "bad >>> cat\n",
      "\n",
      "Item #4:\n",
      "  name goodness\n",
      "0  cat      bad >>> cat\n",
      "\n",
      "Item #5:\n",
      "('bad', 'good') >>> cat\n",
      "\n",
      "Item #6:\n",
      "['bad', 'good'] >>> cat\n",
      "\n",
      "Item #7:\n",
      "<function func_key at 0x12846c160> >>> cat\n",
      "\n",
      "Item #8:\n",
      "<function <lambda> at 0x1291c0160> >>> cat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get()\n",
    "assert stash.get(df_key) == \"cat\"\n",
    "assert stash.get('fake_key') == None\n",
    "\n",
    "# __contains__\n",
    "assert df_key in stash\n",
    "\n",
    "# __len__\n",
    "assert len(stash) == 8   # from earlier\n",
    "\n",
    "# keys()\n",
    "from hashstash import *\n",
    "for i,key in enumerate(stash.keys()): \n",
    "    pass\n",
    "\n",
    "# values()\n",
    "for value in stash.values():\n",
    "    assert value == \"cat\"\n",
    "\n",
    "# items()\n",
    "for i, (key, value) in enumerate(stash.items()):\n",
    "    print(f'Item #{i+1}:\\n{key} >>> {value}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other dictionary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop()\n",
    "assert stash.pop(df_key) == \"cat\"\n",
    "assert df_key not in stash\n",
    "\n",
    "# setdefault()\n",
    "assert stash.setdefault(df_key, \"new_cat_default\") == \"new_cat_default\"\n",
    "assert stash.get(df_key) == \"new_cat_default\"\n",
    "\n",
    "# update()\n",
    "another_dict = {'new_key_of_badness': 'cat'}\n",
    "stash.update(another_dict)\n",
    "assert stash['new_key_of_badness'] == \"cat\"\n",
    "\n",
    "# update() with another stash\n",
    "another_stash = HashStash(engine='memory').clear()\n",
    "another_stash[[1,2,3]] = \"cat\"\n",
    "stash.update(another_stash)\n",
    "assert stash[[1,2,3]] == \"cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under the hood\n",
    "\n",
    "You can also iterate the keys and values as actually exist in the data store, i.e. serialized encoded:\n",
    "\n",
    "- `_keys()`: Return an iterator over the encoded keys\n",
    "\n",
    "- `_values()`: Return an iterator over the encoded values\n",
    "\n",
    "- `_items()`: Return an iterator over the encoded key-value pai\n",
    "\n",
    "These methods are used internally and not necessary to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterating over ._items():\n",
      "b'NwAAAPETeyJfX3B5X18iOiAiYnVpbHRpbnMuc2V0IiwgIl9fZGF0YRwA8AFbImdvb2QiLCAiYmFkIl19' is the serialized, compressed, and encoded key for b'BQAAAFAiY2F0Ig=='\n",
      "{'good', 'bad'} is the decoded, uncompressed, and deserialized key for cat\n"
     ]
    }
   ],
   "source": [
    "print('\\nIterating over ._items():')\n",
    "for encoded_key,encoded_value in stash._items():\n",
    "    print(encoded_key, 'is the serialized, compressed, and encoded key for', encoded_value)\n",
    "    decoded_key = stash.decode_key(encoded_key)\n",
    "    decoded_value = stash.decode_value(encoded_value)\n",
    "    print(decoded_key, 'is the decoded, uncompressed, and deserialized key for', decoded_value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stashing function results\n",
    "\n",
    "\n",
    "HashStash provides two ways of stashing results.\n",
    "\n",
    "First, here's an expensive function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing expensive_computation time #1\n",
      "Executing expensive_computation time #2\n"
     ]
    }
   ],
   "source": [
    "# Here's an expensive function\n",
    "\n",
    "num_times_computed = 0\n",
    "\n",
    "def expensive_computation(names,goodnesses=['good']):\n",
    "    import random\n",
    "    global num_times_computed\n",
    "    num_times_computed += 1\n",
    "    print(f'Executing expensive_computation time #{num_times_computed}')\n",
    "    ld=[]\n",
    "    for n in range(1_000_000):\n",
    "        d={}\n",
    "        d['name']=random.choice(names)\n",
    "        d['goodness']=random.choice(goodnesses)\n",
    "        d['random']=random.random()\n",
    "        ld.append(d)\n",
    "    return random.sample(ld,k=10)\n",
    "\n",
    "names = ['cat', 'dog']\n",
    "goodnesses=['good','bad']\n",
    "\n",
    "# execute 2 times -- different results\n",
    "unstashed_result1 = expensive_computation(names, goodnesses=goodnesses)\n",
    "unstashed_result2 = expensive_computation(names, goodnesses=goodnesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Stashing function results via `stash.run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing expensive_computation time #3\n"
     ]
    }
   ],
   "source": [
    "## set up a stash to run the function in\n",
    "functions_stash = HashStash('functions_stash', clear=True)\n",
    "\n",
    "# execute time #3\n",
    "stashed_result1 = functions_stash.run(expensive_computation, names, goodnesses=goodnesses)\n",
    "\n",
    "# calls #4-5 will not execute but return stashed result\n",
    "stashed_result2 = functions_stash.run(expensive_computation, names, goodnesses=goodnesses)\n",
    "stashed_result3 = functions_stash.run(expensive_computation, names, goodnesses=goodnesses)\n",
    "assert stashed_result1 == stashed_result2 == stashed_result3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Using function decorator `@stash.stashed_result`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing expensive_computation time #4\n"
     ]
    }
   ],
   "source": [
    "from hashstash import stashed_result\n",
    "\n",
    "@functions_stash.stashed_result  # or @stashed_result(\"functions_stash\") [same HashStash call args/kwargs]\n",
    "def expensive_computation2(names, goodnesses=['good']):\n",
    "    return expensive_computation(names, goodnesses=goodnesses)\n",
    "\n",
    "# will run once\n",
    "stashed_result4 = expensive_computation2(names, goodnesses=goodnesses)\n",
    "\n",
    "# then cached even when calling it normally\n",
    "stashed_result5 = expensive_computation2(names, goodnesses=goodnesses)\n",
    "stashed_result6 = expensive_computation2(names, goodnesses=goodnesses)\n",
    "assert stashed_result4 == stashed_result5 == stashed_result6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing function result stash\n",
    "Once a function is stashed via either the methods above you can access its stash as an attribute of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function results cached in LMDBHashStash(~/.cache/hashstash/functions_stash/lmdb.hashstash.lz4/stashed_result/__main__.expensive_computation/lmdb.hashstash.lz4/data.db)\n",
      "\n",
      "Stashed key = ((['cat', 'dog'],), {'goodnesses': ['good', 'bad']})\n",
      "Called args: (['cat', 'dog'],)\n",
      "Called kwargs: {'goodnesses': ['good', 'bad']}\n",
      "\n",
      "Stashed value = [{'name': 'dog', 'goodness': 'bad', 'random': 0.5057600020943653}, {'name': 'dog', 'goodness': 'bad', 'random': 0.44942716869985244}, {'name': 'dog', 'goodness': 'bad', 'random': 0.04412090932878976}, {'name': 'dog', 'goodness': 'good', 'random': 0.26390218890484296}, {'name': 'dog', 'goodness': 'good', 'random': 0.8861568169357764}, {'name': 'dog', 'goodness': 'bad', 'random': 0.8113840172104607}, {'name': 'dog', 'goodness': 'bad', 'random': 0.29450288091375965}, {'name': 'cat', 'goodness': 'good', 'random': 0.10650085474589033}, {'name': 'dog', 'goodness': 'bad', 'random': 0.10346094332240874}, {'name': 'cat', 'goodness': 'bad', 'random': 0.29552371113906584}]\n"
     ]
    }
   ],
   "source": [
    "# function now has .stash attribute, from either method\n",
    "func_stash = expensive_computation.stash\n",
    "func_stash2 = expensive_computation2.stash\n",
    "assert len(func_stash) == len(func_stash2)\n",
    "print(f'Function results cached in {func_stash}\\n')\n",
    "\n",
    "# can iterate over its results normally. Keys are: (args as tuple, kwargs as dict)\n",
    "func_stash = func_stash2\n",
    "for key, value in func_stash.items():\n",
    "    args, kwargs = key\n",
    "    print(f'Stashed key = {key}')\n",
    "    print(f'Called args: {args}')\n",
    "    print(f'Called kwargs: {kwargs}')\n",
    "    print(f'\\nStashed value = {value}')\n",
    "\n",
    "# you can get result via normal get\n",
    "stashed_result7 = func_stash.get(((names,), {'goodnesses':goodnesses}))\n",
    "\n",
    "# or via special get_func function which accepts function call syntax\n",
    "stashed_result8 = func_stash.get_func(names, goodnesses=goodnesses)\n",
    "\n",
    "assert stashed_result7 == stashed_result8 == stashed_result5 == stashed_result6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping functions\n",
    "\n",
    "You can also map functions across many objects, with stashed results, with `stash.map`. By default it uses {num_proc}-2 processors to start computing results in background. In the meantime it returns a `StashMap` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mMapping __main__.expensive_computation3 across 4 objects [2x]\u001b[0m:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StashMap([StashMapRun(__main__.expensive_computation3('cat', goodnesses=['good', 'bad']) >>> ?),\n",
       "          StashMapRun(__main__.expensive_computation3('dog', goodnesses=['good', 'bad']) >>> ?),\n",
       "          StashMapRun(__main__.expensive_computation3('aardvark', goodnesses=['good', 'bad']) >>> ?),\n",
       "          StashMapRun(__main__.expensive_computation3('zebra', goodnesses=['good', 'bad']) >>> ?)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expensive_computation3(name, goodnesses=['good']):\n",
    "    time.sleep(random.randint(1,5))\n",
    "    return {'name':name, 'goodness':random.choice(goodnesses)}\n",
    "\n",
    "# this returns a custom StashMap object instantly, computing results in background (if num_proc>1)\n",
    "stash_map = functions_stash.map(expensive_computation3, ['cat','dog','aardvark','zebra'], goodnesses=['good', 'bad'], num_proc=2)\n",
    "stash_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mMapping __main__.expensive_computation3 across 4 objects [2x]\u001b[0m:  50%|█████     | 2/4 [00:05<00:04,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+5.0] {'name': 'cat', 'goodness': 'good'}\n",
      "[+5.0] {'name': 'dog', 'goodness': 'good'}\n",
      "[+5.0] {'name': 'aardvark', 'goodness': 'good'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mMapping __main__.expensive_computation3 across 4 objects [2x]\u001b[0m: 100%|██████████| 4/4 [00:09<00:00,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+9.0] {'name': 'zebra', 'goodness': 'bad'}\n"
     ]
    }
   ],
   "source": [
    "# iterate over results as they come in:\n",
    "timestart=time.time()\n",
    "for result in stash_map.results_iter():\n",
    "    print(f'[+{time.time()-timestart:.1f}] {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'cat', 'goodness': 'good'},\n",
       " {'name': 'dog', 'goodness': 'good'},\n",
       " {'name': 'aardvark', 'goodness': 'good'},\n",
       " {'name': 'zebra', 'goodness': 'bad'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or wait for as a list\n",
    "stash_map.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat',) {'goodnesses': ['good', 'bad']} >>> {'name': 'cat', 'goodness': 'good'}\n",
      "('dog',) {'goodnesses': ['good', 'bad']} >>> {'name': 'dog', 'goodness': 'good'}\n",
      "('aardvark',) {'goodnesses': ['good', 'bad']} >>> {'name': 'aardvark', 'goodness': 'good'}\n",
      "('zebra',) {'goodnesses': ['good', 'bad']} >>> {'name': 'zebra', 'goodness': 'bad'}\n"
     ]
    }
   ],
   "source": [
    "# or by .items() or .keys() or .values()\n",
    "for (args,kwargs),result in stash_map.items():\n",
    "    print(f'{args} {kwargs} >>> {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mMapping __main__.expensive_computation3 across 4 objects [2x]\u001b[0m:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StashMap([StashMapRun(__main__.expensive_computation3('cat', goodnesses=['good', 'bad']) >>> ?),\n",
       "          StashMapRun(__main__.expensive_computation3('dog', goodnesses=['good', 'bad']) >>> ?),\n",
       "          StashMapRun(__main__.expensive_computation3('elephant', goodnesses=['good', 'bad']) >>> ?),\n",
       "          StashMapRun(__main__.expensive_computation3('donkey', goodnesses=['good', 'bad']) >>> ?)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the next time, it will return stashed results, and compute only new values\n",
    "stash_map2 = functions_stash.map(expensive_computation3, ['cat','dog','elephant','donkey'], goodnesses=['good', 'bad'], num_proc=2)\n",
    "stash_map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# heavily customizable\n",
    "stash_map3 = functions_stash.map(\n",
    "    expensive_computation3, \n",
    "    objects=['cat','parrot'],               # (2 new animals\n",
    "    options=[{'goodnesses':['bad']}, {}],   # list of dictionaries for specific keyword arguments\n",
    "    goodnesses=['good', 'bad'],             # keyword arguments common to all function calls\n",
    "    num_proc=4,                             # number of processes to use\n",
    "    preload=True,                           # start loading stashed results on init\n",
    "    precompute=True,                        # start computing stashed results \n",
    "    progress=True,                          # show progress bar\n",
    "    desc=\"Mapping expensive_computation3\",  # description for progress bar\n",
    "    ordered=True,                           # maintain order of input\n",
    "    stash_runs=True,                        # store individual function runs\n",
    "    stash_map=True,                         # store the entire map result\n",
    "    _force=False,                           # don't force recomputation if results exist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StashMap([StashMapRun(__main__.expensive_computation4('mole', root_dir='function_stash') >>> {'name': 'mole', 'goodness': 'good'}),\n",
       "          StashMapRun(__main__.expensive_computation4('lizard', root_dir='function_stash') >>> {'name': 'lizard', 'goodness': 'good'}),\n",
       "          StashMapRun(__main__.expensive_computation4('turkey', root_dir='function_stash') >>> {'name': 'turkey', 'goodness': 'good'})])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also use as a decorator\n",
    "\n",
    "@stash_mapped('function_stash', num_proc=1)\n",
    "def expensive_computation4(name, goodnesses=['good']):\n",
    "    time.sleep(random.randint(1,5))\n",
    "    return {'name':name, 'goodness':random.choice(goodnesses)}\n",
    "\n",
    "expensive_computation4(['mole','lizard','turkey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling DataFrames\n",
    "\n",
    "HashStash can assemble DataFrames from cached contents, even nested ones. First, examples from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name goodness    random\n",
      "0  dog      bad  0.505760\n",
      "1  dog      bad  0.449427\n",
      "2  dog      bad  0.044121\n",
      "3  dog     good  0.263902\n",
      "4  dog     good  0.886157\n",
      "5  dog      bad  0.811384\n",
      "6  dog      bad  0.294503\n",
      "7  cat     good  0.106501\n",
      "8  dog      bad  0.103461\n",
      "9  cat      bad  0.295524\n"
     ]
    }
   ],
   "source": [
    "# assemble list of flattened dictionaries from cached contents\n",
    "func_stash.ld                # or stash.assemble_ld()\n",
    "\n",
    "# assemble dataframe from flattened dictionaries of cached contents\n",
    "print(func_stash.df)         # or stash.assemble_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested data flattening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           name goodness  etc.age  etc.goes_to.heaven\n",
      "_key                                                 \n",
      "Animal 1    cat     good        9                True\n",
      "Animal 2    cat      bad        8               False\n",
      "Animal 3    cat     good        6                True\n",
      "Animal 4    dog      bad        7                True\n",
      "Animal 5    dog      bad       10                True\n",
      "...         ...      ...      ...                 ...\n",
      "Animal 96   dog      bad        2                True\n",
      "Animal 97   dog      bad        8                True\n",
      "Animal 98   cat      bad        9               False\n",
      "Animal 99   cat     good        5                True\n",
      "Animal 100  cat     good        9                True\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# can also work with nested data\n",
    "nested_data_stash = HashStash(engine='memory', dbname='assembling_dfs')\n",
    "\n",
    "# populate stash with random animals\n",
    "import random\n",
    "for n in range(100):\n",
    "    nested_data_stash[f'Animal {n+1}'] = {\n",
    "        'name': (cat_or_dog := random.choice(['cat', 'dog'])), \n",
    "        'goodness': (goodness := random.choice(['good', 'bad'])),\n",
    "        'etc': {\n",
    "            'age': random.randint(1, 10),\n",
    "            'goes_to':{\n",
    "                'heaven':True if cat_or_dog=='dog' or goodness=='good' else False,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# assemble dataframe from flattened dictionaries of cached contents\n",
    "print(nested_data_stash.df)         # or stash.assemble_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append mode\n",
    "\n",
    "Keep track of all versions of a key/value pair. All engines can track version number; \"pairtree\" tracks timestamp as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest value: {'goodness': 'bad'}\n",
      "All values: [{'goodness': 'good'}, {'goodness': 'bad'}]\n",
      "All values with metadata: [{'_version': 1, '_timestamp': 1725652978.878733, '_value': {'goodness': 'good'}}, {'_version': 2, '_timestamp': 1725652978.878886, '_value': {'goodness': 'bad'}}]\n"
     ]
    }
   ],
   "source": [
    "append_stash = HashStash(\"readme_append_mode\", engine='pairtree', append_mode=True, clear=True)\n",
    "key = {\"name\":\"cat\"}\n",
    "append_stash[key] = {\"goodness\": \"good\"}\n",
    "append_stash[key] = {\"goodness\": \"bad\"}\n",
    "\n",
    "print(f'Latest value: {append_stash.get(key)}')\n",
    "print(f'All values: {append_stash.get_all(key)}')\n",
    "print(f'All values with metadata: {append_stash.get_all(key, with_metadata=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also get metadata on dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      name goodness\n",
      "_version _timestamp                \n",
      "1        1.725653e+09  cat     good\n",
      "2        1.725653e+09  cat      bad\n"
     ]
    }
   ],
   "source": [
    "print(append_stash.assemble_df(with_metadata=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Caches\n",
    "\n",
    "HashStash provides a `tmp` method for creating temporary caches that are automatically cleaned up. The temporary cache is automatically cleared and removed after the with block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "with stash.tmp() as tmp_stash:\n",
    "    tmp_stash[\"key\"] = \"value\"\n",
    "    print(\"key\" in tmp_stash)\n",
    "    \n",
    "print(\"key\" in tmp_stash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialization\n",
    "\n",
    "HashStash supports multiple serialization methods:\n",
    "\n",
    "- `serialize`: Serializes Python objects\n",
    "- `deserialize`: Deserializes data back into Python objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashstash import serialize, deserialize\n",
    "\n",
    "data = pd.DataFrame({\"name\": [\"cat\", \"dog\"], \"goodness\": [\"good\", \"bad\"]})\n",
    "serialized_data = serialize(data, serializer=\"hashstash\") # or jsonpickle or pickle\n",
    "deserialized_data = deserialize(serialized_data, serializer=\"hashstash\")\n",
    "\n",
    "data.equals(deserialized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Compression\n",
    "\n",
    "HashStash provides functions for encoding and compressing data:\n",
    "\n",
    "- `encode`: Encodes and optionally compresses data\n",
    "- `decode`: Decodes and decompresses data\n",
    "\n",
    "These functions are used internally by HashStash but can also be used directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mMapping __main__.expensive_computation3 across 4 objects [2x]\u001b[0m: 6it [00:04,  1.45it/s]               "
     ]
    }
   ],
   "source": [
    "from hashstash import encode, decode\n",
    "\n",
    "data = b\"Hello, World!\"\n",
    "encoded_data = encode(data, compress='lz4', b64=True)\n",
    "decoded_data = decode(encoded_data, compress='lz4', b64=True)\n",
    "\n",
    "data == decoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "### Engines\n",
    "\n",
    "LMDB is the fastest engine, followed by the custom \"pairtree\" implementation.\n",
    "\n",
    "![Engines](./figures/fig.comparing_engines.png)\n",
    "\n",
    "### Serializers\n",
    "\n",
    "Pickle is by far the fastest serializer, but it is not transportable between Python versions. HashStash is generally faster than jsonpickle, and can serialize more data types (including lambdas and functions within functions), but it produces larger file sizes.\n",
    "\n",
    "![Serializers](./figures/fig.comparing_serializers_size_speed.png)\n",
    "\n",
    "### Encodings\n",
    "\n",
    "LZ4 is the fastest compressor, but it requires an external dependency. BZ2 is the slowest, but it provides the best compression ratio.\n",
    "\n",
    "![Compressors](./figures/fig.comparing_encodings_size_speed.png)\n",
    "\n",
    "### All together\n",
    "\n",
    "LMDB engine, with pickle serializer, with no compression (raw) or LZ4 or blosc compression is the fastest combination of parameters; followed by pairtree with the same. \n",
    "\n",
    "![All together](./figures/fig.comparing_engines_serializers_encodings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "### Tests\n",
    "\n",
    "To run the tests, clone this repository and run  `pytest` in the root project directory.\n",
    "\n",
    "### Contributing\n",
    "\n",
    "Contributions are welcome! Please feel free to submit a Pull Request.\n",
    "\n",
    "### License\n",
    "\n",
    "This project is licensed under the GNU License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
